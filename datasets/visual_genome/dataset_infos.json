{"region-descriptions": {"description": "Visual Genome enable to model objects and relationshipt between objects.\nThey collect dense annotations of objects, attributes, and relationships within each image.\nSpecifically, the dataset contains over 108K images where each image has an average of 35 objects, 26 attributes, and 21 pairwise relationships between objects.\n", "citation": "@inproceedings{krishnavisualgenome,\n  title={Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations},\n  author={Krishna, Ranjay and Zhu, Yuke and Groth, Oliver and Johnson, Justin and Hata, Kenji and Kravitz, Joshua and Chen, Stephanie and Kalantidis, Yannis and Li, Li-Jia and Shamma, David A and Bernstein, Michael and Fei-Fei, Li},\n  year = {2016},\n  url = {https://arxiv.org/abs/1602.07332},\n}\n", "homepage": "https://visualgenome.org/", "license": "Creative Commons Attribution 4.0 International License", "features": {"image_id": {"dtype": "int32", "id": null, "_type": "Value"}, "url": {"dtype": "string", "id": null, "_type": "Value"}, "width": {"dtype": "int32", "id": null, "_type": "Value"}, "height": {"dtype": "int32", "id": null, "_type": "Value"}, "coco_id": {"dtype": "int64", "id": null, "_type": "Value"}, "flickr_id": {"dtype": "int64", "id": null, "_type": "Value"}, "regions": {"feature": {"region_id": {"dtype": "int32", "id": null, "_type": "Value"}, "image_id": {"dtype": "int32", "id": null, "_type": "Value"}, "phrase": {"dtype": "string", "id": null, "_type": "Value"}, "x": {"dtype": "int32", "id": null, "_type": "Value"}, "y": {"dtype": "int32", "id": null, "_type": "Value"}, "width": {"dtype": "int32", "id": null, "_type": "Value"}, "height": {"dtype": "int32", "id": null, "_type": "Value"}}, "length": -1, "id": null, "_type": "Sequence"}}, "post_processed": null, "supervised_keys": null, "task_templates": null, "builder_name": "visual_genome", "config_name": "region-descriptions", "version": {"version_str": "1.2.0", "description": null, "major": 1, "minor": 2, "patch": 0}, "splits": {"train": {"name": "train", "num_bytes": 306241636, "num_examples": 108077, "dataset_name": "visual_genome"}}, "download_checksums": {"https://visualgenome.org/static/data/dataset/image_data.json.zip": {"num_bytes": 1780854, "checksum": "b87a94918cb2ff4d952cf1dfeca0b9cf6cd6fd204c2f8704645653be1163681a"}, "https://visualgenome.org/static/data/dataset/region_descriptions.json.zip": {"num_bytes": 127377968, "checksum": "038382219029c9cb0d7b0308b1d45f7537cf5a47b98f493f6d0e30c8947fb15d"}}, "download_size": 129158822, "post_processing_size": null, "dataset_size": 306241636, "size_in_bytes": 435400458}, "question-answering": {"description": "Visual Genome enable to model objects and relationshipt between objects.\nThey collect dense annotations of objects, attributes, and relationships within each image.\nSpecifically, the dataset contains over 108K images where each image has an average of 35 objects, 26 attributes, and 21 pairwise relationships between objects.\n", "citation": "@inproceedings{krishnavisualgenome,\n  title={Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations},\n  author={Krishna, Ranjay and Zhu, Yuke and Groth, Oliver and Johnson, Justin and Hata, Kenji and Kravitz, Joshua and Chen, Stephanie and Kalantidis, Yannis and Li, Li-Jia and Shamma, David A and Bernstein, Michael and Fei-Fei, Li},\n  year = {2016},\n  url = {https://arxiv.org/abs/1602.07332},\n}\n", "homepage": "https://visualgenome.org/", "license": "Creative Commons Attribution 4.0 International License", "features": {"image_id": {"dtype": "int32", "id": null, "_type": "Value"}, "url": {"dtype": "string", "id": null, "_type": "Value"}, "width": {"dtype": "int32", "id": null, "_type": "Value"}, "height": {"dtype": "int32", "id": null, "_type": "Value"}, "coco_id": {"dtype": "int64", "id": null, "_type": "Value"}, "flickr_id": {"dtype": "int64", "id": null, "_type": "Value"}, "qas": {"feature": {"qa_id": {"dtype": "int32", "id": null, "_type": "Value"}, "image_id": {"dtype": "int32", "id": null, "_type": "Value"}, "question": {"dtype": "string", "id": null, "_type": "Value"}, "answer": {"dtype": "string", "id": null, "_type": "Value"}, "a_objects": {"feature": {"object_id": {"dtype": "int32", "id": null, "_type": "Value"}, "x": {"dtype": "int32", "id": null, "_type": "Value"}, "y": {"dtype": "int32", "id": null, "_type": "Value"}, "w": {"dtype": "int32", "id": null, "_type": "Value"}, "h": {"dtype": "int32", "id": null, "_type": "Value"}, "name": {"dtype": "string", "id": null, "_type": "Value"}, "syntets": {"feature": {"dtype": "string", "id": null, "_type": "Value"}, "length": -1, "id": null, "_type": "Sequence"}}, "length": -1, "id": null, "_type": "Sequence"}, "q_objects": {"feature": {"object_id": {"dtype": "int32", "id": null, "_type": "Value"}, "x": {"dtype": "int32", "id": null, "_type": "Value"}, "y": {"dtype": "int32", "id": null, "_type": "Value"}, "w": {"dtype": "int32", "id": null, "_type": "Value"}, "h": {"dtype": "int32", "id": null, "_type": "Value"}, "name": {"dtype": "string", "id": null, "_type": "Value"}, "syntets": {"feature": {"dtype": "string", "id": null, "_type": "Value"}, "length": -1, "id": null, "_type": "Sequence"}}, "length": -1, "id": null, "_type": "Sequence"}}, "length": -1, "id": null, "_type": "Sequence"}}, "post_processed": null, "supervised_keys": null, "task_templates": null, "builder_name": "visual_genome", "config_name": "question-answering", "version": {"version_str": "1.2.0", "description": null, "major": 1, "minor": 2, "patch": 0}, "splits": {"train": {"name": "train", "num_bytes": 172573991, "num_examples": 108077, "dataset_name": "visual_genome"}}, "download_checksums": {"https://visualgenome.org/static/data/dataset/image_data.json.zip": {"num_bytes": 1780854, "checksum": "b87a94918cb2ff4d952cf1dfeca0b9cf6cd6fd204c2f8704645653be1163681a"}, "https://visualgenome.org/static/data/dataset/question_answers.json.zip": {"num_bytes": 24776315, "checksum": "ba5ef71a5852d9fc7e066a8af05711fce58901b2152e9fd636efdf49fd1fc613"}}, "download_size": 26557169, "post_processing_size": null, "dataset_size": 172573991, "size_in_bytes": 199131160}}